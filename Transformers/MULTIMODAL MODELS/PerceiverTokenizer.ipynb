{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PerceiverTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"deepmind/language-perceiver\"\n",
    "sequence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PerceiverTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerceiverTokenizer(name_or_path='deepmind/language-perceiver', vocab_size=262, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: PerceiverTokenizer = PerceiverTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## special ids and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 0, 4, 3]\n",
      "['[BOS]', '[EOS]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'q',\n",
       " 'u',\n",
       " 'i',\n",
       " 'c',\n",
       " 'k',\n",
       " ' ',\n",
       " 'b',\n",
       " 'r',\n",
       " 'o',\n",
       " 'w',\n",
       " 'n',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'x',\n",
       " ' ',\n",
       " 'j',\n",
       " 'u',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'g',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_ids 将分词后的token映射为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90,\n",
       " 110,\n",
       " 107,\n",
       " 38,\n",
       " 119,\n",
       " 123,\n",
       " 111,\n",
       " 105,\n",
       " 113,\n",
       " 38,\n",
       " 104,\n",
       " 120,\n",
       " 117,\n",
       " 125,\n",
       " 116,\n",
       " 38,\n",
       " 108,\n",
       " 117,\n",
       " 126,\n",
       " 38,\n",
       " 112,\n",
       " 123,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 38,\n",
       " 117,\n",
       " 124,\n",
       " 107,\n",
       " 120,\n",
       " 38,\n",
       " 122,\n",
       " 110,\n",
       " 107,\n",
       " 38,\n",
       " 114,\n",
       " 103,\n",
       " 128,\n",
       " 127,\n",
       " 38,\n",
       " 106,\n",
       " 117,\n",
       " 109,\n",
       " 52]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode = tokenize + convert_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90, 110, 107, 38, 119, 123, 111, 105, 113, 38, 104, 120, 117, 125, 116, 38, 108, 117, 126, 38, 112, 123, 115, 118, 121, 38, 117, 124, 107, 120, 38, 122, 110, 107, 38, 114, 103, 128, 127, 38, 106, 117, 109, 52]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 90,\n",
       " 110,\n",
       " 107,\n",
       " 38,\n",
       " 119,\n",
       " 123,\n",
       " 111,\n",
       " 105,\n",
       " 113,\n",
       " 38,\n",
       " 104,\n",
       " 120,\n",
       " 117,\n",
       " 125,\n",
       " 116,\n",
       " 38,\n",
       " 108,\n",
       " 117,\n",
       " 126,\n",
       " 38,\n",
       " 112,\n",
       " 123,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 38,\n",
       " 117,\n",
       " 124,\n",
       " 107,\n",
       " 120,\n",
       " 38,\n",
       " 122,\n",
       " 110,\n",
       " 107,\n",
       " 38,\n",
       " 114,\n",
       " 103,\n",
       " 128,\n",
       " 127,\n",
       " 38,\n",
       " 106,\n",
       " 117,\n",
       " 109,\n",
       " 52,\n",
       " 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode(sequence, add_special_tokens=False))\n",
    "ids = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode前后代表开始和结束\n",
    "tokenizer.convert_ids_to_tokens([4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_encode_plus = batch encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[4, 90, 110, 107, 38, 119, 123, 111, 105, 113, 38, 104, 120, 117, 125, 116, 38, 108, 117, 126, 38, 112, 123, 115, 118, 121, 38, 117, 124, 107, 120, 38, 122, 110, 107, 38, 114, 103, 128, 127, 38, 106, 117, 109, 52, 5]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_ids_to_tokens 将数字映射为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'q',\n",
       " 'u',\n",
       " 'i',\n",
       " 'c',\n",
       " 'k',\n",
       " ' ',\n",
       " 'b',\n",
       " 'r',\n",
       " 'o',\n",
       " 'w',\n",
       " 'n',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'x',\n",
       " ' ',\n",
       " 'j',\n",
       " 'u',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'g',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_string 将token转换为string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode = convert_ids_to_tokens + convert_tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]The quick brown fox jumps over the lazy dog.[SEP]\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))\n",
    "print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode = batch decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]The quick brown fox jumps over the lazy dog.[SEP]']\n",
      "['The quick brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode([ids]))\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4,  90, 110,  ...,   0,   0,   0]], device='cuda:0')\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
      "tensor([2048], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequence,  # 单个句子\n",
    "    truncation=True,  # 超出max_length截断处理\n",
    "    padding=\"max_length\",  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens=True,  # text添加特殊key\n",
    "    return_length=True,  # 返回有效长度\n",
    "    return_overflowing_tokens=False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])  # 对应文字id\n",
    "print(inputs[\"attention_mask\"])  # 对应是否是文字\n",
    "print(inputs[\"length\"])  # 对应总长度长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[  4,  90, 110, 107,  38, 119, 123, 111, 105, 113,  38, 104, 120, 117,\n",
      "         125, 116,  38, 108, 117, 126,  38, 112, 123, 115, 118, 121,  38, 117,\n",
      "         124, 107, 120,  38, 122, 110, 107,  38, 114, 103, 128, 127,  38, 106,\n",
      "         117, 109,  52,   5],\n",
      "        [  4,  90, 110, 107,  38, 119, 123, 111, 105, 113,  38, 104, 120, 117,\n",
      "         125, 116,  38, 108, 117, 126,  38, 112, 123, 115, 118, 121,  38, 117,\n",
      "         124, 107, 120,  38, 122, 110, 107,  38, 114, 103, 128, 127,  38, 106,\n",
      "         117, 109,  52,   5]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([46, 46], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,  # 句子batch\n",
    "    truncation=True,  # 超出max_length截断处理\n",
    "    padding=True,  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens=True,  # text添加特殊key\n",
    "    return_length=True,  # 返回有效长度\n",
    "    return_overflowing_tokens=False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(\n",
    "    inputs[\"input_ids\"]\n",
    ")  # 对应文字id 添加了['<|startoftext|>', '<|endoftext|>'],和encode相同\n",
    "print(inputs[\"attention_mask\"])  # 对应是否是文字\n",
    "print(inputs[\"length\"])  # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
