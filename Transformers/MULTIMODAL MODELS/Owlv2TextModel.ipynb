{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2TextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"google/owlv2-base-patch16-ensemble\"\n",
    "texts = [\"a photo of 2 cats\", \"a photo of a dog\", \"a plane in the blue sky\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Owlv2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Owlv2Processor:\n",
       "- image_processor: Owlv2ImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Owlv2ImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"Owlv2Processor\",\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 960,\n",
       "    \"width\": 960\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='google/owlv2-base-patch16-ensemble', vocab_size=49408, model_max_length=16, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor: Owlv2Processor = Owlv2Processor.from_pretrained(version)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1125,   539,   273,  3989, 49407,     0],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407,     0],\n",
       "        [49406,   320,  5363,   530,   518,  1746,  2390, 49407]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text=texts,  # 可以为列表或单个string\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    "    padding=True,  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,  # 如果使用max_length要将padding设置为 \"max_length\"\n",
    "    add_special_tokens=True,  # text添加特殊key\n",
    ").to(device, torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   320,  1125,   539,   273,  3989, 49407,     0],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407,     0],\n",
       "        [49406,   320,  5363,   530,   518,  1746,  2390, 49407]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftext|>a photo of 2 cats <|endoftext|>!', '<|startoftext|>a photo of a dog <|endoftext|>!', '<|startoftext|>a plane in the blue sky <|endoftext|>']\n",
      "['a photo of 2 cats', 'a photo of a dog', 'a plane in the blue sky']\n"
     ]
    }
   ],
   "source": [
    "print(processor.batch_decode(inputs[\"input_ids\"]))\n",
    "print(processor.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>a photo of 2 cats <|endoftext|>!\n",
      "a photo of 2 cats\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode(inputs[\"input_ids\"][0]))\n",
    "print(processor.decode(inputs[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Owlv2TextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Owlv2TextModel(\n",
       "  (text_model): Owlv2TextTransformer(\n",
       "    (embeddings): Owlv2TextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(16, 512)\n",
       "    )\n",
       "    (encoder): Owlv2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Owlv2EncoderLayer(\n",
       "          (self_attn): Owlv2Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Owlv2MLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model: Owlv2TextModel = Owlv2TextModel.from_pretrained(\n",
    "    version, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-1.8305e-02,  2.3669e-02,  2.3141e-02,  ..., -2.8261e-01,\n",
       "           1.5751e-01, -4.0075e-01],\n",
       "         [ 5.8490e-01,  6.6315e-01,  1.1305e+00,  ..., -9.7600e-01,\n",
       "           4.5851e-01, -6.4424e-01],\n",
       "         [-2.4948e-01,  4.0482e-01,  1.5877e+00,  ...,  8.9683e-01,\n",
       "           8.3434e-01, -1.5613e+00],\n",
       "         ...,\n",
       "         [ 8.0426e-01,  4.5553e-01, -7.7922e-02,  ..., -1.1559e+00,\n",
       "           1.5640e+00, -9.3029e-02],\n",
       "         [ 1.3706e+00,  1.4272e+00,  5.0517e-01,  ..., -5.6414e-01,\n",
       "           9.8100e-01,  3.6156e-01],\n",
       "         [ 1.5388e+00,  1.1661e+00,  5.8623e-01,  ..., -6.0757e-01,\n",
       "           4.8703e-01,  4.3506e-01]],\n",
       "\n",
       "        [[-1.8305e-02,  2.3669e-02,  2.3141e-02,  ..., -2.8261e-01,\n",
       "           1.5751e-01, -4.0075e-01],\n",
       "         [ 5.8490e-01,  6.6315e-01,  1.1305e+00,  ..., -9.7600e-01,\n",
       "           4.5851e-01, -6.4424e-01],\n",
       "         [-2.4948e-01,  4.0482e-01,  1.5877e+00,  ...,  8.9683e-01,\n",
       "           8.3434e-01, -1.5613e+00],\n",
       "         ...,\n",
       "         [-4.9140e-01, -1.1026e+00, -8.8552e-01,  ..., -1.6541e+00,\n",
       "           5.3807e-01,  2.0974e-01],\n",
       "         [ 6.7271e-01, -4.8095e-01,  4.6079e-02,  ..., -5.2030e-01,\n",
       "           1.1842e+00,  4.6459e-01],\n",
       "         [ 6.9474e-01, -5.2481e-01,  3.2093e-01,  ..., -5.6590e-01,\n",
       "           6.1604e-01,  3.9698e-01]],\n",
       "\n",
       "        [[-1.8305e-02,  2.3669e-02,  2.3141e-02,  ..., -2.8261e-01,\n",
       "           1.5751e-01, -4.0075e-01],\n",
       "         [ 5.8490e-01,  6.6315e-01,  1.1305e+00,  ..., -9.7600e-01,\n",
       "           4.5851e-01, -6.4424e-01],\n",
       "         [ 3.5915e-01, -2.0194e+00,  1.0832e+00,  ..., -1.1230e+00,\n",
       "          -4.2103e-01,  4.9531e-01],\n",
       "         ...,\n",
       "         [ 7.7596e-01, -7.6222e-01,  3.8570e-01,  ...,  8.1684e-01,\n",
       "          -8.3799e-01, -3.2692e-01],\n",
       "         [-3.2581e-01, -2.1412e-01, -1.1540e+00,  ...,  6.7884e-01,\n",
       "          -6.9740e-02,  5.8300e-01],\n",
       "         [ 1.4508e-01,  2.8473e-05,  8.8562e-01,  ...,  4.3918e-01,\n",
       "           1.8592e-01,  8.1706e-01]]], device='cuda:0'), pooler_output=tensor([[ 1.3706e+00,  1.4272e+00,  5.0517e-01,  ..., -5.6414e-01,\n",
       "          9.8100e-01,  3.6156e-01],\n",
       "        [ 6.7271e-01, -4.8095e-01,  4.6079e-02,  ..., -5.2030e-01,\n",
       "          1.1842e+00,  4.6459e-01],\n",
       "        [ 1.4508e-01,  2.8473e-05,  8.8562e-01,  ...,  4.3918e-01,\n",
       "          1.8592e-01,  8.1706e-01]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = text_model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一层的输出\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文字长度进行pool\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
