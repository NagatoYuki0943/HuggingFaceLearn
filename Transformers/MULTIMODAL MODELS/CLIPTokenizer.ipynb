{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"openai/clip-vit-base-patch32\"\n",
    "sequence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizer(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## special ids and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49406, 49407, 49407]\n",
      "['<|startoftext|>', '<|endoftext|>', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize(sequence) 对文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the</w>',\n",
       " 'quick</w>',\n",
       " 'brown</w>',\n",
       " 'fox</w>',\n",
       " 'jumps</w>',\n",
       " 'over</w>',\n",
       " 'the</w>',\n",
       " 'lazy</w>',\n",
       " 'dog</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_ids 将分词后的token映射为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[518, 3712, 2866, 3240, 18911, 962, 518, 10753, 1929, 269]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode = tokenize + convert_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[518, 3712, 2866, 3240, 18911, 962, 518, 10753, 1929, 269]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[49406, 518, 3712, 2866, 3240, 18911, 962, 518, 10753, 1929, 269, 49407]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode(sequence, add_special_tokens=False))\n",
    "ids = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftext|>', '<|endoftext|>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode前后代表开始和结束\n",
    "tokenizer.convert_ids_to_tokens([49406, 49407])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_encode_plus = batch encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[49406, 518, 3712, 2866, 3240, 18911, 962, 518, 10753, 1929, 269, 49407]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_ids_to_tokens 将数字映射为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftext|>', 'the</w>', 'quick</w>', 'brown</w>', 'fox</w>', 'jumps</w>', 'over</w>', 'the</w>', 'lazy</w>', 'dog</w>', '.</w>', '<|endoftext|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the</w>',\n",
       " 'quick</w>',\n",
       " 'brown</w>',\n",
       " 'fox</w>',\n",
       " 'jumps</w>',\n",
       " 'over</w>',\n",
       " 'the</w>',\n",
       " 'lazy</w>',\n",
       " 'dog</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_string 将token转换为string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumps over the lazy dog .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode = convert_ids_to_tokens + convert_tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>the quick brown fox jumps over the lazy dog. <|endoftext|>\n",
      "the quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))\n",
    "print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode = batch decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftext|>the quick brown fox jumps over the lazy dog. <|endoftext|>']\n",
      "['the quick brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode([ids]))\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequence,                           # 单个句子\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应总长度长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407],\n",
      "        [49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,                     # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id 添加了['<|startoftext|>', '<|endoftext|>'],和encode相同\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: CLIPTokenizerFast = CLIPTokenizerFast.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407],\n",
      "        [49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,                     # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id 添加了['<|startoftext|>', '<|endoftext|>'],和encode相同\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 返回文字长度时最长长度,结果不对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoTokenizer 通用封装，根据载入预训练模型来自适应\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/autoclass_tutorial#autotokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizer(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                                                               默认为Fast,可以不用\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(version, use_fast=False)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407],\n",
      "        [49406,   518,  3712,  2866,  3240, 18911,   962,   518, 10753,  1929,\n",
      "           269, 49407]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,                     # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id 添加了['<|startoftext|>', '<|endoftext|>'],和encode相同\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 返回文字最长长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
