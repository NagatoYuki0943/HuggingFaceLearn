{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Data2VecAudioForXVector, AutoProcessor, AutoFeatureExtractor\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"facebook/data2vec-audio-base-960h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/ImageBind#usage\n",
    "\n",
    "For windows users, you might need to install librosa and soundfile for reading/writing audio files. (Thanks @congyue1977)\n",
    "\n",
    "`pip install soundfile librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 73\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:/Users/Administrator/.cache/huggingface/datasets/downloads/extracted/b49df5cb4e26d70a35c542fbe0eadc8bfee0f971809886d2131859668faeba1c/dev_clean/1272/128104\\\\1272-128104-0000.flac',\n",
       " 'audio': {'path': 'C:/Users/Administrator/.cache/huggingface/datasets/downloads/extracted/b49df5cb4e26d70a35c542fbe0eadc8bfee0f971809886d2131859668faeba1c/dev_clean/1272/128104\\\\1272-128104-0000.flac',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "         0.0010376 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " 'speaker_id': 1272,\n",
       " 'chapter_id': 128104,\n",
       " 'id': '1272-128104-0000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "        0.0010376 ]),\n",
       " array([-1.52587891e-04, -9.15527344e-05, -1.83105469e-04, ...,\n",
       "         9.76562500e-04,  9.46044922e-04, -4.88281250e-04])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get multi array\n",
    "[d[\"array\"] for d in dataset[:2][\"audio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " \"NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get multi text\n",
    "[d for d in dataset[:2][\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='facebook/data2vec-audio-base-960h', vocab_size=32, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor: AutoProcessor = AutoProcessor.from_pretrained(version)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0386,  0.0337,  0.0322,  ...,  0.0070,  0.0095,  0.0169],\n",
       "        [-0.0015, -0.0008, -0.0019,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    [d[\"array\"] for d in dataset[:2][\"audio\"]],\n",
    "    sampling_rate=sampling_rate,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device, torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 93680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '',\n",
       " 'E',\n",
       " 'T',\n",
       " 'A',\n",
       " 'O',\n",
       " 'N',\n",
       " 'I',\n",
       " 'H',\n",
       " 'S',\n",
       " 'R',\n",
       " 'D',\n",
       " 'L',\n",
       " 'U',\n",
       " 'M',\n",
       " 'W',\n",
       " 'C',\n",
       " 'F',\n",
       " 'G',\n",
       " 'Y',\n",
       " 'P',\n",
       " 'B',\n",
       " 'V',\n",
       " 'K',\n",
       " \"'\",\n",
       " 'X',\n",
       " 'J',\n",
       " 'Q',\n",
       " 'Z',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(range(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data2VecAudioForXVector\n",
    "\n",
    "Data2VecAudio Model with an XVector feature extraction head on top for tasks like Speaker Verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Data2VecAudioForXVector were not initialized from the model checkpoint at facebook/data2vec-audio-base-960h and are newly initialized: ['tdnn.1.kernel.bias', 'tdnn.2.kernel.weight', 'tdnn.0.kernel.bias', 'tdnn.3.kernel.bias', 'feature_extractor.weight', 'tdnn.0.kernel.weight', 'tdnn.4.kernel.bias', 'tdnn.1.kernel.weight', 'tdnn.2.kernel.bias', 'tdnn.4.kernel.weight', 'classifier.weight', 'objective.weight', 'projector.weight', 'feature_extractor.bias', 'projector.bias', 'classifier.bias', 'tdnn.3.kernel.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data2VecAudioForXVector(\n",
       "  (data2vec_audio): Data2VecAudioModel(\n",
       "    (feature_extractor): Data2VecAudioFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Data2VecAudioConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Data2VecAudioConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Data2VecAudioConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Data2VecAudioFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Data2VecAudioEncoder(\n",
       "      (pos_conv_embed): Data2VecAudioPositionalConvEmbedding(\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x Data2VecAudioPositionalConvLayer(\n",
       "            (conv): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)\n",
       "            (padding): Data2VecAudioPadLayer()\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Data2VecAudioEncoderLayer(\n",
       "          (attention): Data2VecAudioAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Data2VecAudioFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (tdnn): ModuleList(\n",
       "    (0): TDNNLayer(\n",
       "      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1-2): 2 x TDNNLayer(\n",
       "      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (objective): AMSoftmaxLoss(\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model: Data2VecAudioForXVector = Data2VecAudioForXVector.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XVectorOutput(loss=None, logits=tensor([[-0.0009,  0.0011, -0.0018,  ..., -0.0008,  0.0022,  0.0040],\n",
       "        [-0.0010,  0.0012, -0.0016,  ..., -0.0008,  0.0023,  0.0041]],\n",
       "       device='cuda:0'), embeddings=tensor([[ 0.0009,  0.0027,  0.0124,  ...,  0.0003,  0.0084, -0.0014],\n",
       "        [ 0.0010,  0.0031,  0.0123,  ...,  0.0005,  0.0079, -0.0017]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "tensor([[-0.0009,  0.0011, -0.0018,  ..., -0.0008,  0.0022,  0.0040],\n",
      "        [-0.0010,  0.0012, -0.0016,  ..., -0.0008,  0.0023,  0.0041]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "tensor([[ 0.0009,  0.0027,  0.0124,  ...,  0.0003,  0.0084, -0.0014],\n",
      "        [ 0.0010,  0.0031,  0.0123,  ...,  0.0005,  0.0079, -0.0017]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(outputs.embeddings.shape)\n",
    "print(outputs.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0094,  0.0286,  0.1313,  ...,  0.0027,  0.0884, -0.0152],\n",
       "        [ 0.0108,  0.0332,  0.1313,  ...,  0.0048,  0.0845, -0.0180]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准化\n",
    "torch.nn.functional.normalize(outputs.embeddings, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
