{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModelWithProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"openai/clip-vit-base-patch32\"\n",
    "sequence = [\"a photo of 2 cats\", \"a photo of a dog\", \"a plane in the blue sky\"]\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizer(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[49406,   320,  1125,   539,   273,  3989, 49407, 49407],\n",
      "        [49406,   320,  1125,   539,   320,  1929, 49407, 49407],\n",
      "        [49406,   320,  5363,   530,   518,  1746,  2390, 49407]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([7, 7, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequence,                           # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device, torch.float16)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id 添加了['<|startoftext|>', '<|endoftext|>'],和encode相同\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPTextModelWithProjection\n",
    "\n",
    "CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModelWithProjection(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model: CLIPTextModelWithProjection = CLIPTextModelWithProjection.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModelOutput(text_embeds=tensor([[ 0.3023,  0.0104, -0.6035,  ..., -0.5600, -0.2901, -0.1535],\n",
       "        [ 0.0932,  0.2764, -0.4137,  ..., -0.5852, -0.2590,  0.1193],\n",
       "        [ 0.0693, -0.1149, -0.2575,  ..., -0.0537, -0.0485,  0.0403]],\n",
       "       device='cuda:0'), last_hidden_state=tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 1.3097, -0.8255,  1.2024,  ...,  0.3621,  0.1972, -1.5122],\n",
       "         [-0.0836, -0.2263,  0.3341,  ...,  0.2727, -0.2437, -1.7002],\n",
       "         [-0.1426, -0.2440,  0.3692,  ...,  0.2555, -0.2330, -1.7032]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [-0.1433, -0.5163,  1.7099,  ..., -0.0795,  0.3609, -1.2437],\n",
       "         [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016],\n",
       "         [ 0.0656, -0.0023,  1.2809,  ..., -0.3969, -0.3909, -1.3407]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 0.4590,  0.5516, -0.0744,  ..., -0.3672,  2.0714,  0.1245],\n",
       "         ...,\n",
       "         [ 0.6469, -1.0562, -1.9888,  ...,  0.0578,  0.6242, -1.4064],\n",
       "         [ 0.9853, -0.7531, -0.8554,  ...,  0.6692,  2.0735,  0.7088],\n",
       "         [ 0.8121, -0.9096, -0.7681,  ...,  0.0974,  0.3438,  0.2209]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = text_model(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask = inputs[\"attention_mask\"],\n",
    "    )\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "tensor([[ 0.3023,  0.0104, -0.6035,  ..., -0.5600, -0.2901, -0.1535],\n",
      "        [ 0.0932,  0.2764, -0.4137,  ..., -0.5852, -0.2590,  0.1193],\n",
      "        [ 0.0693, -0.1149, -0.2575,  ..., -0.0537, -0.0485,  0.0403]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 投影后的结果\n",
    "# get_text_features的结果\n",
    "print(outputs.text_embeds.shape)\n",
    "print(outputs.text_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一层的输出,和CLIPTextModel结果相同\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
