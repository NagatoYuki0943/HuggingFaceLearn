{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ChineseCLIPProcessor, ChineseCLIPTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"OFA-Sys/chinese-clip-vit-base-patch16\"\n",
    "max_length = 20\n",
    "text = [\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChineseCLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChineseCLIPProcessor:\n",
       "- image_processor: ChineseCLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": false,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ChineseCLIPFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"ChineseCLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: BertTokenizerFast(name_or_path='OFA-Sys/chinese-clip-vit-base-patch16', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor: ChineseCLIPProcessor = ChineseCLIPProcessor.from_pretrained(version)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3345, 2225, 7991,  102,    0],\n",
       "        [ 101, 1975, 6032, 4905, 2094,  102],\n",
       "        [ 101, 2207, 4125, 7987,  102,    0],\n",
       "        [ 101, 4649, 1305,  687,  102,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text = text,                # 可以为列表或单个string\n",
    "    return_tensors = \"pt\",      # 返回数据格式 np pt tf jax\n",
    "    padding = True,             # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,  # 如果使用max_length要将padding设置为 \"max_length\"\n",
    "    add_special_tokens = True,  # text添加特殊key\n",
    ").to(device, torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3345, 2225, 7991,  102,    0],\n",
       "        [ 101, 1975, 6032, 4905, 2094,  102],\n",
       "        [ 101, 2207, 4125, 7987,  102,    0],\n",
       "        [ 101, 4649, 1305,  687,  102,    0]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPTextModel\n",
    "\n",
    "The text model from CHINESE_CLIP without any head or projection on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at OFA-Sys/chinese-clip-vit-base-patch16 were not used when initializing ChineseCLIPTextModel: ['text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layer.9.output.dense.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layer.5.attention.output.dense.weight', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layer.10.output.dense.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layer.9.output.dense.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.0.intermediate.dense.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layer.6.attention.self.key.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layer.4.output.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layer.11.attention.self.value.weight', 'vision_model.embeddings.position_ids', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.query.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.intermediate.dense.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.output.dense.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layer.11.intermediate.dense.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.embeddings.word_embeddings.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layer.10.output.dense.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layer.6.output.dense.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.0.output.dense.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'vision_model.pre_layrnorm.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layer.3.output.dense.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.5.intermediate.dense.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.embeddings.LayerNorm.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layer.11.attention.self.query.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.11.output.dense.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layer.5.output.dense.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layer.4.attention.self.value.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layer.0.output.dense.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.post_layernorm.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.embeddings.position_embeddings.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layer.1.output.dense.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'visual_projection.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layer.3.intermediate.dense.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.4.output.dense.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.embeddings.token_type_embeddings.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.3.output.dense.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layer.2.output.dense.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layer.2.output.dense.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layer.1.attention.self.value.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ChineseCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ChineseCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ChineseCLIPTextModel were not initialized from the model checkpoint at OFA-Sys/chinese-clip-vit-base-patch16 and are newly initialized: ['encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.bias', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChineseCLIPTextModel(\n",
       "  (embeddings): ChineseCLIPTextEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): ChineseCLIPTextEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ChineseCLIPTextLayer(\n",
       "        (attention): ChineseCLIPTextAttention(\n",
       "          (self): ChineseCLIPTextSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ChineseCLIPTextSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ChineseCLIPTextIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ChineseCLIPTextOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): ChineseCLIPTextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model: ChineseCLIPTextModel = ChineseCLIPTextModel.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.2313e-01, -7.0572e-01,  1.8726e-01,  ...,  7.4099e-01,\n",
       "           1.1313e+00, -7.1933e-01],\n",
       "         [-1.0530e+00, -4.2926e-01, -1.1825e-01,  ..., -6.3781e-01,\n",
       "          -6.3158e-01, -6.6794e-01],\n",
       "         [ 1.0290e+00, -6.0097e-02, -1.2589e+00,  ...,  1.2395e+00,\n",
       "          -5.7168e-01, -1.4174e-01],\n",
       "         [-3.3115e-01, -6.0512e-01, -5.8363e-01,  ..., -1.2125e-01,\n",
       "           3.3261e-02, -6.6432e-01],\n",
       "         [ 4.2290e-01, -2.2424e-01, -7.1796e-01,  ...,  3.1229e-01,\n",
       "           2.2947e-01,  4.7062e-01],\n",
       "         [-1.2025e+00,  3.4781e-01, -6.4434e-02,  ..., -9.3520e-02,\n",
       "           1.1780e+00, -1.3417e+00]],\n",
       "\n",
       "        [[ 7.7450e-01, -5.5769e-01,  7.1401e-01,  ...,  9.1673e-01,\n",
       "           1.2666e+00, -3.2234e-01],\n",
       "         [ 2.0000e-02,  3.1104e-01,  3.4901e-01,  ...,  7.3439e-01,\n",
       "           1.7366e-01, -7.1819e-01],\n",
       "         [ 3.5890e-01,  7.2860e-01,  1.1539e+00,  ...,  7.1729e-01,\n",
       "          -8.2187e-01, -4.6041e-01],\n",
       "         [ 3.1449e-01, -1.7261e-01, -4.6147e-01,  ...,  9.2496e-02,\n",
       "           1.2268e+00, -2.0380e-01],\n",
       "         [ 6.3721e-01,  7.7021e-01, -1.4896e+00,  ..., -5.0509e-01,\n",
       "           1.7694e-01,  1.5118e-01],\n",
       "         [ 1.9421e-01,  1.5649e-01,  3.8516e-01,  ...,  3.3504e-01,\n",
       "           1.0313e+00, -5.5820e-01]],\n",
       "\n",
       "        [[ 4.1842e-01, -1.0468e+00,  3.2507e-01,  ...,  8.2988e-01,\n",
       "           1.1017e+00, -5.3488e-01],\n",
       "         [-1.6313e+00, -1.2075e+00, -4.0051e-01,  ...,  3.5412e-01,\n",
       "          -1.3116e+00, -3.2885e-01],\n",
       "         [ 3.3562e-02, -2.5929e-01, -1.4102e+00,  ...,  2.6362e-01,\n",
       "          -9.5198e-01, -3.8041e-01],\n",
       "         [-8.9507e-01, -5.4397e-01, -8.4984e-01,  ..., -3.7726e-01,\n",
       "           6.0411e-01, -5.4607e-01],\n",
       "         [ 6.8072e-01, -7.3796e-01, -7.4691e-01,  ...,  4.0270e-01,\n",
       "           4.8900e-01,  7.5855e-01],\n",
       "         [-1.0383e+00, -2.3904e-01,  4.2014e-04,  ..., -1.6962e-01,\n",
       "           1.2528e+00, -1.1663e+00]],\n",
       "\n",
       "        [[ 6.4075e-01, -6.4763e-01,  6.3919e-01,  ...,  7.7080e-01,\n",
       "           1.0451e+00, -5.5081e-01],\n",
       "         [-6.2351e-01, -6.6057e-01,  5.5557e-01,  ..., -3.0047e-01,\n",
       "          -6.2487e-01, -7.8305e-01],\n",
       "         [-5.7604e-01,  4.0624e-01, -5.0799e-01,  ...,  2.3290e-01,\n",
       "          -3.6589e-01, -2.7342e-01],\n",
       "         [-1.0459e+00, -5.2873e-01, -2.7726e-01,  ..., -3.2728e-01,\n",
       "           1.1282e+00, -4.3841e-02],\n",
       "         [ 6.1239e-01, -4.2939e-01, -5.1912e-01,  ...,  2.6882e-01,\n",
       "           3.9992e-01,  4.9165e-01],\n",
       "         [-1.0865e+00,  2.7184e-01,  3.2559e-01,  ..., -9.6175e-02,\n",
       "           1.2293e+00, -1.0915e+00]]], device='cuda:0'), pooler_output=tensor([[ 0.5727,  0.5454,  0.5758,  ..., -0.6879, -0.1607, -0.1729],\n",
       "        [ 0.5795,  0.6584,  0.4052,  ..., -0.7576,  0.1150, -0.4997],\n",
       "        [ 0.5822,  0.6448,  0.5676,  ..., -0.5391, -0.0260, -0.4372],\n",
       "        [ 0.6193,  0.7348,  0.2566,  ..., -0.6738, -0.0313, -0.2575]],\n",
       "       device='cuda:0'), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = text_model(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask = inputs[\"attention_mask\"],\n",
    "    )\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一层的输出\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文字长度进行pool\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
