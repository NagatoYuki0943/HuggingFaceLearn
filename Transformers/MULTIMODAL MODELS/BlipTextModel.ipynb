{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"Salesforce/blip-image-captioning-base\"\n",
    "text = [\"a photo of 2 cats\", \"a photo of a dog\", \"a plane in the blue sky\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlipProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipProcessor:\n",
       "- image_processor: BlipImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"BlipImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"BlipProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 384,\n",
       "    \"width\": 384\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: BertTokenizerFast(name_or_path='Salesforce/blip-image-captioning-base', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor: BlipProcessor = BlipProcessor.from_pretrained(version)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1037, 6302, 1997, 1016, 8870,  102,    0],\n",
       "        [ 101, 1037, 6302, 1997, 1037, 3899,  102,    0],\n",
       "        [ 101, 1037, 4946, 1999, 1996, 2630, 3712,  102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text = text,                # 可以为列表或单个string\n",
    "    return_tensors = \"pt\",      # 返回数据格式 np pt tf jax\n",
    "    padding = True,             # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,  # 如果使用max_length要将padding设置为 \"max_length\"\n",
    "    add_special_tokens = True,  # text添加特殊key\n",
    ").to(device, torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1037, 6302, 1997, 1016, 8870,  102,    0],\n",
       "        [ 101, 1037, 6302, 1997, 1037, 3899,  102,    0],\n",
       "        [ 101, 1037, 4946, 1999, 1996, 2630, 3712,  102]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] a photo of 2 cats [SEP] [PAD]', '[CLS] a photo of a dog [SEP] [PAD]', '[CLS] a plane in the blue sky [SEP]']\n",
      "['a photo of 2 cats', 'a photo of a dog', 'a plane in the blue sky']\n"
     ]
    }
   ],
   "source": [
    "print(processor.batch_decode(inputs[\"input_ids\"]))\n",
    "print(processor.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a photo of 2 cats [SEP] [PAD]\n",
      "a photo of 2 cats\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode(inputs[\"input_ids\"][0]))\n",
    "print(processor.decode(inputs[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlipTextModel\n",
    "\n",
    "The model can behave as an encoder (with only self-attention) as well as a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['encoder.layer.5.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipTextModel(\n",
       "  (embeddings): BlipTextEmbeddings(\n",
       "    (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): BlipTextEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BlipTextLayer(\n",
       "        (attention): BlipTextAttention(\n",
       "          (self): BlipTextSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): BlipTextSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (crossattention): BlipTextAttention(\n",
       "          (self): BlipTextSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): BlipTextSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BlipTextIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BlipTextOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BlipTextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model: BlipTextModel = BlipTextModel.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.2979, -1.7190, -0.8251,  ..., -0.5837,  2.1027,  1.2016],\n",
       "         [-1.1430, -1.1887,  0.7231,  ..., -0.7695, -0.9998,  1.8963],\n",
       "         [-0.3444, -1.5037,  0.2573,  ..., -1.1309,  0.2408,  2.7538],\n",
       "         ...,\n",
       "         [-0.7810, -0.5631, -1.3456,  ..., -1.0601,  0.8383,  2.9879],\n",
       "         [-0.1547,  0.0396,  0.6592,  ...,  0.0644, -0.0628,  2.0471],\n",
       "         [-1.5574,  0.0929,  0.1392,  ..., -1.7714,  1.4565,  2.4659]],\n",
       "\n",
       "        [[-1.8166, -1.3529, -0.7097,  ..., -1.0158,  2.0907,  1.1875],\n",
       "         [-0.8335, -1.0432,  0.7825,  ..., -1.0618, -0.7747,  1.9698],\n",
       "         [ 0.0682, -1.1981,  0.2835,  ..., -1.4119,  0.1731,  2.5956],\n",
       "         ...,\n",
       "         [ 0.3892, -1.7910,  0.1197,  ...,  0.0439,  0.8506,  3.1678],\n",
       "         [ 0.0994,  0.3958,  0.7491,  ..., -0.2503, -0.1343,  1.8826],\n",
       "         [-1.0241,  0.4199,  0.1813,  ..., -2.0382,  1.3635,  2.2894]],\n",
       "\n",
       "        [[-1.6700, -0.9050, -0.9313,  ..., -0.2866,  2.5493,  0.7205],\n",
       "         [-0.8092, -0.1810,  0.7365,  ..., -0.4836, -0.5429,  1.4690],\n",
       "         [-0.0363, -0.7901,  1.1018,  ..., -1.5069,  0.3905,  0.4013],\n",
       "         ...,\n",
       "         [-0.3986, -0.6352, -0.9352,  ..., -0.1681,  2.1320,  2.6320],\n",
       "         [-0.5734,  0.6397,  1.5184,  ...,  0.3664, -0.4337, -0.8071],\n",
       "         [ 0.4801,  1.7351, -0.9832,  ..., -0.7332,  2.1827,  2.4730]]],\n",
       "       device='cuda:0'), pooler_output=tensor([[-0.3303, -0.2122, -0.6773,  ..., -0.1871, -0.4480,  0.3251],\n",
       "        [-0.3847, -0.0428, -0.6028,  ..., -0.1261, -0.3806,  0.3708],\n",
       "        [-0.6907,  0.1572, -0.3644,  ..., -0.3504, -0.2086,  0.6092]],\n",
       "       device='cuda:0'), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = text_model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一层的输出\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文字长度进行pool\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
