{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ReformerTokenizer, ReformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"google/reformer-crime-and-punishment\"\n",
    "sequence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerTokenizer(name_or_path='google/reformer-crime-and-punishment', vocab_size=320, model_max_length=524288, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: ReformerTokenizer = ReformerTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## special ids and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0]\n",
      "['</s>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize(sequence) 对文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁The',\n",
       " '▁qu',\n",
       " 'i',\n",
       " 'ck',\n",
       " '▁b',\n",
       " 'r',\n",
       " 'ow',\n",
       " 'n',\n",
       " '▁f',\n",
       " 'o',\n",
       " 'x',\n",
       " '▁',\n",
       " 'j',\n",
       " 'um',\n",
       " 'p',\n",
       " 's',\n",
       " '▁o',\n",
       " 'ver',\n",
       " '▁the',\n",
       " '▁l',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " '▁do',\n",
       " 'g',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_ids 将分词后的token映射为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 243,\n",
       " 264,\n",
       " 134,\n",
       " 17,\n",
       " 267,\n",
       " 77,\n",
       " 263,\n",
       " 22,\n",
       " 262,\n",
       " 297,\n",
       " 258,\n",
       " 304,\n",
       " 177,\n",
       " 279,\n",
       " 266,\n",
       " 14,\n",
       " 89,\n",
       " 13,\n",
       " 35,\n",
       " 261,\n",
       " 299,\n",
       " 272,\n",
       " 137,\n",
       " 275,\n",
       " 278]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode = tokenize + convert_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140, 243, 264, 134, 17, 267, 77, 263, 22, 262, 297, 258, 304, 177, 279, 266, 14, 89, 13, 35, 261, 299, 272, 137, 275, 278]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 243,\n",
       " 264,\n",
       " 134,\n",
       " 17,\n",
       " 267,\n",
       " 77,\n",
       " 263,\n",
       " 22,\n",
       " 262,\n",
       " 297,\n",
       " 258,\n",
       " 304,\n",
       " 177,\n",
       " 279,\n",
       " 266,\n",
       " 14,\n",
       " 89,\n",
       " 13,\n",
       " 35,\n",
       " 261,\n",
       " 299,\n",
       " 272,\n",
       " 137,\n",
       " 275,\n",
       " 278]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode(sequence, add_special_tokens=False))\n",
    "ids = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ere', '▁S']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_encode_plus = batch encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[140, 243, 264, 134, 17, 267, 77, 263, 22, 262, 297, 258, 304, 177, 279, 266, 14, 89, 13, 35, 261, 299, 272, 137, 275, 278]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_ids_to_tokens 将数字映射为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁The', '▁qu', 'i', 'ck', '▁b', 'r', 'ow', 'n', '▁f', 'o', 'x', '▁', 'j', 'um', 'p', 's', '▁o', 'ver', '▁the', '▁l', 'a', 'z', 'y', '▁do', 'g', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁The',\n",
       " '▁qu',\n",
       " 'i',\n",
       " 'ck',\n",
       " '▁b',\n",
       " 'r',\n",
       " 'ow',\n",
       " 'n',\n",
       " '▁f',\n",
       " 'o',\n",
       " 'x',\n",
       " '▁',\n",
       " 'j',\n",
       " 'um',\n",
       " 'p',\n",
       " 's',\n",
       " '▁o',\n",
       " 'ver',\n",
       " '▁the',\n",
       " '▁l',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " '▁do',\n",
       " 'g',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_string 将token转换为string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode = convert_ids_to_tokens + convert_tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))\n",
    "print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode = batch decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog.']\n",
      "['The quick brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode([ids]))\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[140, 243, 264, 134,  17, 267,  77, 263,  22, 262, 297, 258, 304, 177,\n",
      "         279, 266,  14,  89,  13,  35, 261, 299, 272, 137, 275, 278]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequence,                           # 单个句子\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_attention_mask = True,       # 返回attention_mask\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\",              # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应总长度长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[140, 243, 264, 134,  17, 267,  77, 263,  22, 262, 297, 258, 304, 177,\n",
      "         279, 266,  14,  89,  13,  35, 261, 299, 272, 137, 275, 278],\n",
      "        [140, 243, 264, 134,  17, 267,  77, 263,  22, 262, 297, 258, 304, 177,\n",
      "         279, 266,  14,  89,  13,  35, 261, 299, 272, 137, 275, 278]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "tensor([26, 26], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,                     # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_attention_mask = True,       # 返回attention_mask\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\",              # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerTokenizerFast(name_or_path='google/reformer-crime-and-punishment', vocab_size=320, model_max_length=524288, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: ReformerTokenizerFast = ReformerTokenizerFast.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[140, 243, 264, 134,  17, 267,  77, 263,  22, 262, 297, 258, 304, 177,\n",
      "         279, 266,  14,  89,  13,  35, 261, 299, 272, 137, 275, 278],\n",
      "        [140, 243, 264, 134,  17, 267,  77, 263,  22, 262, 297, 258, 304, 177,\n",
      "         279, 266,  14,  89,  13,  35, 261, 299, 272, 137, 275, 278]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "tensor([26, 26], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,                     # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    # padding = True,                   # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_attention_mask = True,       # 返回attention_mask\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\",              # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 返回文字长度时最长长度,结果不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
