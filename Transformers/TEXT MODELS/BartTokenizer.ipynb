{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"facebook/bart-base\"\n",
    "sequence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizer(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: BartTokenizer = BartTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## special ids and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 1, 50264]\n",
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize(sequence) 对文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Ġquick',\n",
       " 'Ġbrown',\n",
       " 'Ġfox',\n",
       " 'Ġjumps',\n",
       " 'Ġover',\n",
       " 'Ġthe',\n",
       " 'Ġlazy',\n",
       " 'Ġdog',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_ids 将分词后的token映射为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[133, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode = tokenize + convert_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 133, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 4, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode(sequence, add_special_tokens=False))\n",
    "ids = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 没有开始的token\n",
    "tokenizer.convert_ids_to_tokens([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_encode_plus = batch encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 133, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_ids_to_tokens 将数字映射为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Ġquick',\n",
       " 'Ġbrown',\n",
       " 'Ġfox',\n",
       " 'Ġjumps',\n",
       " 'Ġover',\n",
       " 'Ġthe',\n",
       " 'Ġlazy',\n",
       " 'Ġdog',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_tokens_to_string 将token转换为string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode = convert_ids_to_tokens + convert_tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The quick brown fox jumps over the lazy dog.</s>\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))\n",
    "print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_decode = batch decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>The quick brown fox jumps over the lazy dog.</s>']\n",
      "['The quick brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode([ids]))\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
      "             4,     2]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([12])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequence,  # 单个句子\n",
    "    truncation=True,  # 超出max_length截断处理\n",
    "    padding=True,  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens=True,  # text添加特殊key\n",
    "    return_length=True,  # 返回有效长度\n",
    "    return_overflowing_tokens=False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    ")\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])  # 对应文字id\n",
    "print(inputs[\"attention_mask\"])  # 对应是否是文字\n",
    "print(inputs[\"length\"])  # 对应总长度长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'length', 'attention_mask'])\n",
      "tensor([[    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
      "             4,     2],\n",
      "        [    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
      "             4,     2]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,  # 句子batch\n",
    "    truncation=True,  # 超出max_length截断处理\n",
    "    padding=True,  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    return_length=True,  # 返回有效长度\n",
    "    return_overflowing_tokens=False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])\n",
    "print(inputs[\"attention_mask\"])  # 对应是否是文字\n",
    "print(inputs[\"length\"])  # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: BartTokenizerFast = BartTokenizerFast.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
      "             4,     2],\n",
      "        [    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
      "             4,     2]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([12, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [sequence] * 2,  # 句子batch\n",
    "    truncation=True,  # 超出max_length截断处理\n",
    "    padding=True,  # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens=True,  # text添加特殊key\n",
    "    return_length=True,  # 返回有效长度\n",
    "    return_overflowing_tokens=False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors=\"pt\",  # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])\n",
    "print(inputs[\"attention_mask\"])  # 对应是否是文字\n",
    "print(inputs[\"length\"])  # 返回文字长度时最长长度,结果不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
