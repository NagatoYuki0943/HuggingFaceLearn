{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://so.gushiwen.cn/shiwenv_d75a706935de.aspx\n",
    "# 九月九日忆山东兄弟\n",
    "# 王维〔唐代〕\n",
    "sequences = [\"独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token, tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "tensor([[    1, 29871,   234,   142,   175, 30505,   232,   191,   133, 30574,\n",
      "         30573,   232,   191,   133, 31915, 30214, 31951,   236,   131,   165,\n",
      "           231,   192,   182, 31669,   232,   131,   144, 31579,   231,   189,\n",
      "           181, 30267,   236,   132,   168, 31043,   232,   136,   135,   232,\n",
      "           191,   162, 31451, 30528, 31548, 30214,   236,   132,   144,   233,\n",
      "           146,   149,   235,   143,   180,   235,   147,   187, 31022, 30287,\n",
      "         30313, 30267]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([62])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sequences,                          # 句子\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    max_length = 8192,                  # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ")\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"])      # 对应文字id\n",
    "print(inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(inputs[\"length\"])         # 对应总长度长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = len(sequences[0])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 29871,   234,   142,   175, 30505,   232,   191,   133, 30574,\n",
       "        30573,   232,   191,   133, 31915, 30214, 31951,   236,   131,   165,\n",
       "          231,   192,   182, 31669,   232,   131,   144, 31579,   231,   189,\n",
       "          181, 30267,   236,   132,   168, 31043,   232,   136,   135,   232,\n",
       "          191,   162, 31451, 30528, 31548, 30214,   236,   132,   144,   233,\n",
       "          146,   149,   235,   143,   180,   235,   147,   187, 31022, 30287,\n",
       "        30313, 30267])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = inputs[\"input_ids\"][0]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_len = len(inputs[\"input_ids\"][0])\n",
    "ids_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "�\n",
      "��\n",
      "独\n",
      "独在\n",
      "独在�\n",
      "独在��\n",
      "独在异\n",
      "独在异乡\n",
      "独在异乡为\n",
      "独在异乡为�\n",
      "独在异乡为��\n",
      "独在异乡为异\n",
      "独在异乡为异客\n",
      "独在异乡为异客，\n",
      "独在异乡为异客，每\n",
      "独在异乡为异客，每�\n",
      "独在异乡为异客，每��\n",
      "独在异乡为异客，每逢\n",
      "独在异乡为异客，每����\n",
      "独在异乡为异客，每�����\n",
      "独在异乡为异客，每逢佳\n",
      "独在异乡为异客，每逢佳节\n",
      "独在异乡为异客，每逢佳节�\n",
      "独在异乡为异客，每逢佳节��\n",
      "独在异乡为异客，每逢佳节倍\n",
      "独在异乡为异客，每逢佳节倍思\n",
      "独在异乡为异客，每逢佳节倍思�\n",
      "独在异乡为异客，每逢佳节倍思��\n",
      "独在异乡为异客，每逢佳节倍思亲\n",
      "独在异乡为异客，每逢佳节倍思亲。\n",
      "独在异乡为异客，每逢佳节倍思亲。�\n",
      "独在异乡为异客，每逢佳节倍思亲。��\n",
      "独在异乡为异客，每逢佳节倍思亲。遥\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知�\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知��\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知����\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知�����\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，�\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，��\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，����\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，�����\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，�������\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，��������\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，����������\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，�����������\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。\n"
     ]
    }
   ],
   "source": [
    "for end_id in range(1, ids_len+1):\n",
    "    print(tokenizer.decode(ids[:end_id], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缓存解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "独\n",
      "独在\n",
      "独在异\n",
      "独在异乡\n",
      "独在异乡为\n",
      "独在异乡为异\n",
      "独在异乡为异客\n",
      "独在异乡为异客，\n",
      "独在异乡为异客，每\n",
      "独在异乡为异客，每逢\n",
      "独在异乡为异客，每逢佳\n",
      "独在异乡为异客，每逢佳节\n",
      "独在异乡为异客，每逢佳节倍\n",
      "独在异乡为异客，每逢佳节倍思\n",
      "独在异乡为异客，每逢佳节倍思亲\n",
      "独在异乡为异客，每逢佳节倍思亲。\n",
      "独在异乡为异客，每逢佳节倍思亲。遥\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。\n"
     ]
    }
   ],
   "source": [
    "# 解码失败的token\n",
    "fail_token = \"�\"\n",
    "decode_result = \"\"\n",
    "for end_id in range(1, ids_len+1):\n",
    "    decode_result = tokenizer.decode(ids[:end_id], skip_special_tokens=True)\n",
    "    if fail_token not in decode_result:\n",
    "        print(decode_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缓存解码优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "独\n",
      "独在\n",
      "独在异\n",
      "独在异乡\n",
      "独在异乡为\n",
      "独在异乡为异\n",
      "独在异乡为异客\n",
      "独在异乡为异客，\n",
      "独在异乡为异客，每\n",
      "独在异乡为异客，每逢\n",
      "独在异乡为异客，每逢佳\n",
      "独在异乡为异客，每逢佳节\n",
      "独在异乡为异客，每逢佳节倍\n",
      "独在异乡为异客，每逢佳节倍思\n",
      "独在异乡为异客，每逢佳节倍思亲\n",
      "独在异乡为异客，每逢佳节倍思亲。\n",
      "独在异乡为异客，每逢佳节倍思亲。遥\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人\n",
      "独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。\n"
     ]
    }
   ],
   "source": [
    "# 解码失败的token\n",
    "fail_token = \"�\"\n",
    "decode_result = \"\"\n",
    "start_id = 0\n",
    "for end_id in range(1, ids_len+1):\n",
    "    # 每次只解码最小字词的token\n",
    "    temp_result = tokenizer.decode(ids[start_id:end_id], skip_special_tokens=True)\n",
    "    # print(temp_result)\n",
    "    # 解码一个字/词成功后,start_id 后移,保存解码结果\n",
    "    if fail_token not in temp_result:\n",
    "        start_id = end_id\n",
    "        decode_result += temp_result\n",
    "        print(decode_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
