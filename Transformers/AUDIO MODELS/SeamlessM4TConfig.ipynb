{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import SeamlessM4TModel, SeamlessM4TConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeamlessM4TConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeamlessM4TConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"adaptor_dropout\": 0.1,\n",
       "  \"adaptor_kernel_size\": 8,\n",
       "  \"adaptor_stride\": 8,\n",
       "  \"add_adapter\": true,\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"conv_depthwise_kernel_size\": 31,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 8192,\n",
       "  \"decoder_layerdrop\": 0.05,\n",
       "  \"decoder_layers\": 24,\n",
       "  \"decoder_start_token_id\": 3,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 8192,\n",
       "  \"encoder_layerdrop\": 0.05,\n",
       "  \"encoder_layers\": 24,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"feature_projection_input_dim\": 160,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lang_embed_dim\": 256,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"leaky_relu_slope\": 0.1,\n",
       "  \"max_new_tokens\": 256,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"max_source_positions\": 4096,\n",
       "  \"model_type\": \"seamless_m4t\",\n",
       "  \"num_adapter_layers\": 1,\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_conv_pos_embedding_groups\": 16,\n",
       "  \"num_conv_pos_embeddings\": 128,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embeddings_type\": \"relative\",\n",
       "  \"resblock_dilation_sizes\": [\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ],\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ],\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ]\n",
       "  ],\n",
       "  \"resblock_kernel_sizes\": [\n",
       "    3,\n",
       "    7,\n",
       "    11\n",
       "  ],\n",
       "  \"rotary_embedding_base\": 10000,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"scale_embedding\": true,\n",
       "  \"speech_encoder_attention_heads\": 16,\n",
       "  \"speech_encoder_dropout\": 0.0,\n",
       "  \"speech_encoder_hidden_act\": \"swish\",\n",
       "  \"speech_encoder_intermediate_size\": 4096,\n",
       "  \"speech_encoder_layerdrop\": 0.1,\n",
       "  \"speech_encoder_layers\": 24,\n",
       "  \"spkr_embed_dim\": 256,\n",
       "  \"t2u_bos_token_id\": 0,\n",
       "  \"t2u_decoder_attention_heads\": 16,\n",
       "  \"t2u_decoder_ffn_dim\": 8192,\n",
       "  \"t2u_decoder_layers\": 6,\n",
       "  \"t2u_decoder_start_token_id\": 2,\n",
       "  \"t2u_encoder_attention_heads\": 16,\n",
       "  \"t2u_encoder_ffn_dim\": 8192,\n",
       "  \"t2u_encoder_layers\": 6,\n",
       "  \"t2u_eos_token_id\": 2,\n",
       "  \"t2u_max_new_tokens\": 1024,\n",
       "  \"t2u_max_position_embeddings\": 2048,\n",
       "  \"t2u_pad_token_id\": 1,\n",
       "  \"t2u_vocab_size\": 10082,\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"unit_embed_dim\": 1280,\n",
       "  \"unit_hifi_gan_vocab_size\": 10000,\n",
       "  \"upsample_initial_channel\": 512,\n",
       "  \"upsample_kernel_sizes\": [\n",
       "    11,\n",
       "    8,\n",
       "    8,\n",
       "    4,\n",
       "    4\n",
       "  ],\n",
       "  \"upsample_rates\": [\n",
       "    5,\n",
       "    4,\n",
       "    4,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"use_cache\": true,\n",
       "  \"var_pred_dropout\": 0.5,\n",
       "  \"variance_predictor_kernel_size\": 3,\n",
       "  \"vocab_size\": 256102,\n",
       "  \"vocoder_num_langs\": 36,\n",
       "  \"vocoder_num_spkrs\": 200,\n",
       "  \"vocoder_offset\": 4\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SeamlessM4TConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeamlessM4TModel(\n",
       "  (shared): Embedding(256102, 1024, padding_idx=0)\n",
       "  (text_encoder): SeamlessM4TEncoder(\n",
       "    (embed_tokens): Embedding(256102, 1024, padding_idx=0)\n",
       "    (embed_positions): SeamlessM4TSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x SeamlessM4TEncoderLayer(\n",
       "        (self_attn): SeamlessM4TAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): SeamlessM4TFeedForwardNetwork(\n",
       "          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (speech_encoder): SeamlessM4TSpeechEncoder(\n",
       "    (feature_projection): SeamlessM4TConformerFeatureProjection(\n",
       "      (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SeamlessM4TConformerEncoder(\n",
       "      (embed_positions): SeamlessM4TConformerRelPositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x SeamlessM4TConformerEncoderLayer(\n",
       "          (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn1): SeamlessM4TConformerFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn): SeamlessM4TConformerSelfAttention(\n",
       "            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (conv_module): SeamlessM4TConformerConvolutionModule(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (glu): GLU(dim=1)\n",
       "            (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=same, groups=1024, bias=False)\n",
       "            (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLUActivation()\n",
       "            (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn2): SeamlessM4TConformerFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (intermediate_ffn): SeamlessM4TConformerFeedForward(\n",
       "      (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (intermediate_act_fn): ReLU()\n",
       "      (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (adapter): SeamlessM4TConformerAdapter(\n",
       "      (layers): ModuleList(\n",
       "        (0): SeamlessM4TConformerAdapterLayer(\n",
       "          (residual_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (residual_conv): Conv1d(1024, 2048, kernel_size=(8,), stride=(8,), padding=(4,))\n",
       "          (activation): GLU(dim=1)\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_conv): Conv1d(1024, 2048, kernel_size=(8,), stride=(8,), padding=(4,))\n",
       "          (self_attn): SeamlessM4TConformerSelfAttention(\n",
       "            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): SeamlessM4TConformerFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): ReLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (inner_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): SeamlessM4TDecoder(\n",
       "    (embed_tokens): Embedding(256102, 1024, padding_idx=0)\n",
       "    (embed_positions): SeamlessM4TSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x SeamlessM4TDecoderLayer(\n",
       "        (self_attn): SeamlessM4TAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attention): SeamlessM4TAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (cross_attention_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): SeamlessM4TFeedForwardNetwork(\n",
       "          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256102, bias=False)\n",
       "  (t2u_model): SeamlessM4TTextToUnitForConditionalGeneration(\n",
       "    (model): SeamlessM4TTextToUnitModel(\n",
       "      (encoder): SeamlessM4TEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x SeamlessM4TEncoderLayer(\n",
       "            (self_attn): SeamlessM4TAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): SeamlessM4TFeedForwardNetwork(\n",
       "              (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): SeamlessM4TDecoder(\n",
       "        (embed_tokens): Embedding(10082, 1024, padding_idx=1)\n",
       "        (embed_positions): SeamlessM4TSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x SeamlessM4TDecoderLayer(\n",
       "            (self_attn): SeamlessM4TAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attention): SeamlessM4TAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (cross_attention_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): SeamlessM4TFeedForwardNetwork(\n",
       "              (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=10082, bias=False)\n",
       "  )\n",
       "  (vocoder): SeamlessM4TCodeHifiGan(\n",
       "    (dur_predictor): SeamlessM4TVariancePredictor(\n",
       "      (conv1): Conv1d(1280, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (activation_fuction): ReLU()\n",
       "      (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout_module): Dropout(p=0.5, inplace=False)\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=1280, out_features=1, bias=True)\n",
       "    )\n",
       "    (unit_embedding): Embedding(10000, 1280)\n",
       "    (speaker_embedding): Embedding(200, 256)\n",
       "    (language_embedding): Embedding(36, 256)\n",
       "    (hifi_gan): SeamlessM4THifiGan(\n",
       "      (conv_pre): Conv1d(1792, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (upsampler): ModuleList(\n",
       "        (0): ConvTranspose1d(512, 256, kernel_size=(11,), stride=(5,), padding=(3,))\n",
       "        (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "        (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "        (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        (4): ConvTranspose1d(32, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      )\n",
       "      (resblocks): ModuleList(\n",
       "        (0): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "            (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (2): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "            (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "            (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (3): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "            (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (4): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "            (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (5): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "            (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "            (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (6): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "            (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (7): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "            (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (8): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "            (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "            (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (9): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (10): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (11): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (12): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "            (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (13): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "            (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (14): HifiGanResidualBlock(\n",
       "          (convs1): ModuleList(\n",
       "            (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "            (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "            (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model: SeamlessM4TModel = SeamlessM4TModel(config).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeamlessM4TConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"adaptor_dropout\": 0.1,\n",
       "  \"adaptor_kernel_size\": 8,\n",
       "  \"adaptor_stride\": 8,\n",
       "  \"add_adapter\": true,\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"conv_depthwise_kernel_size\": 31,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 8192,\n",
       "  \"decoder_layerdrop\": 0.05,\n",
       "  \"decoder_layers\": 24,\n",
       "  \"decoder_start_token_id\": 3,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 8192,\n",
       "  \"encoder_layerdrop\": 0.05,\n",
       "  \"encoder_layers\": 24,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"feature_projection_input_dim\": 160,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lang_embed_dim\": 256,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"leaky_relu_slope\": 0.1,\n",
       "  \"max_new_tokens\": 256,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"max_source_positions\": 4096,\n",
       "  \"model_type\": \"seamless_m4t\",\n",
       "  \"num_adapter_layers\": 1,\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_conv_pos_embedding_groups\": 16,\n",
       "  \"num_conv_pos_embeddings\": 128,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embeddings_type\": \"relative\",\n",
       "  \"resblock_dilation_sizes\": [\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ],\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ],\n",
       "    [\n",
       "      1,\n",
       "      3,\n",
       "      5\n",
       "    ]\n",
       "  ],\n",
       "  \"resblock_kernel_sizes\": [\n",
       "    3,\n",
       "    7,\n",
       "    11\n",
       "  ],\n",
       "  \"rotary_embedding_base\": 10000,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"scale_embedding\": true,\n",
       "  \"speech_encoder_attention_heads\": 16,\n",
       "  \"speech_encoder_dropout\": 0.0,\n",
       "  \"speech_encoder_hidden_act\": \"swish\",\n",
       "  \"speech_encoder_intermediate_size\": 4096,\n",
       "  \"speech_encoder_layerdrop\": 0.1,\n",
       "  \"speech_encoder_layers\": 24,\n",
       "  \"spkr_embed_dim\": 256,\n",
       "  \"t2u_bos_token_id\": 0,\n",
       "  \"t2u_decoder_attention_heads\": 16,\n",
       "  \"t2u_decoder_ffn_dim\": 8192,\n",
       "  \"t2u_decoder_layers\": 6,\n",
       "  \"t2u_decoder_start_token_id\": 2,\n",
       "  \"t2u_encoder_attention_heads\": 16,\n",
       "  \"t2u_encoder_ffn_dim\": 8192,\n",
       "  \"t2u_encoder_layers\": 6,\n",
       "  \"t2u_eos_token_id\": 2,\n",
       "  \"t2u_max_new_tokens\": 1024,\n",
       "  \"t2u_max_position_embeddings\": 2048,\n",
       "  \"t2u_pad_token_id\": 1,\n",
       "  \"t2u_vocab_size\": 10082,\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"unit_embed_dim\": 1280,\n",
       "  \"unit_hifi_gan_vocab_size\": 10000,\n",
       "  \"upsample_initial_channel\": 512,\n",
       "  \"upsample_kernel_sizes\": [\n",
       "    11,\n",
       "    8,\n",
       "    8,\n",
       "    4,\n",
       "    4\n",
       "  ],\n",
       "  \"upsample_rates\": [\n",
       "    5,\n",
       "    4,\n",
       "    4,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"use_cache\": true,\n",
       "  \"var_pred_dropout\": 0.5,\n",
       "  \"variance_predictor_kernel_size\": 3,\n",
       "  \"vocab_size\": 256102,\n",
       "  \"vocoder_num_langs\": 36,\n",
       "  \"vocoder_num_spkrs\": 200,\n",
       "  \"vocoder_offset\": 4\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
