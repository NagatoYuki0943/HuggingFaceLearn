{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperForAudioClassification, WhisperFeatureExtractor, WhisperProcessor\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"sanchit-gandhi/whisper-medium-fleurs-lang-id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/ImageBind#usage\n",
    "\n",
    "For windows users, you might need to install librosa and soundfile for reading/writing audio files. (Thanks @congyue1977)\n",
    "\n",
    "`pip install soundfile librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\Administrator\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\hf-internal-testing--librispeech_asr_dummy\\d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b (last modified on Mon Aug  7 16:20:21 2023) since it couldn't be found locally at hf-internal-testing/librispeech_asr_dummy., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 73\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:/Users/Administrator/.cache/huggingface/datasets/downloads/extracted/b49df5cb4e26d70a35c542fbe0eadc8bfee0f971809886d2131859668faeba1c/dev_clean/1272/128104\\\\1272-128104-0000.flac',\n",
       " 'audio': {'path': 'C:/Users/Administrator/.cache/huggingface/datasets/downloads/extracted/b49df5cb4e26d70a35c542fbe0eadc8bfee0f971809886d2131859668faeba1c/dev_clean/1272/128104\\\\1272-128104-0000.flac',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "         0.0010376 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " 'speaker_id': 1272,\n",
       " 'chapter_id': 128104,\n",
       " 'id': '1272-128104-0000'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "        0.0010376 ]),\n",
       " array([-1.52587891e-04, -9.15527344e-05, -1.83105469e-04, ...,\n",
       "         9.76562500e-04,  9.46044922e-04, -4.88281250e-04])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get multi array\n",
    "[d[\"array\"] for d in dataset[:2][\"audio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " \"NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get multi text\n",
    "[d for d in dataset[:2][\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WhisperProcessor 没有对应的模型\n",
    "feature_extractor: WhisperFeatureExtractor = WhisperFeatureExtractor.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[ 1.1933e-01, -9.4576e-02, -1.0978e-01,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01],\n",
       "         [ 4.9347e-04, -8.9271e-02, -6.7290e-02,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01],\n",
       "         [-1.5326e-01, -2.0804e-01, -2.2227e-01,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01],\n",
       "         ...,\n",
       "         [-8.0603e-01, -8.0603e-01, -7.9997e-01,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01],\n",
       "         [-8.0603e-01, -7.7211e-01, -8.0603e-01,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01],\n",
       "         [-8.0603e-01, -8.0603e-01, -8.0603e-01,  ..., -8.0603e-01,\n",
       "          -8.0603e-01, -8.0603e-01]],\n",
       "\n",
       "        [[-4.6956e-01, -7.5109e-02,  2.7610e-02,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01],\n",
       "         [-1.2772e-01, -2.0680e-02, -3.2390e-02,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01],\n",
       "         [-3.1414e-01, -9.7058e-02, -1.8364e-01,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01],\n",
       "         ...,\n",
       "         [-7.0427e-01, -7.0427e-01, -7.0427e-01,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01],\n",
       "         [-7.0427e-01, -7.0427e-01, -7.0427e-01,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01],\n",
       "         [-7.0427e-01, -7.0427e-01, -7.0427e-01,  ..., -7.0427e-01,\n",
       "          -7.0427e-01, -7.0427e-01]]], device='cuda:0')}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(\n",
    "    [d[\"array\"] for d in dataset[:2][\"audio\"]],\n",
    "    sampling_rate=sampling_rate,\n",
    "    # padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device, torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 3000])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_features\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperForAudioClassification(分辨语言种类)\n",
    "\n",
    "Whisper Encoder Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB Keyword Spotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForAudioClassification(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projector): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (classifier): Linear(in_features=256, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model: WhisperForAudioClassification = WhisperForAudioClassification.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8480e+00,  4.7897e-01, -2.3965e+00, -2.4014e+00,  4.5726e-01,\n",
       "         -1.6911e+00, -1.4168e+00,  4.8689e-01, -1.6355e+00,  1.3438e+00,\n",
       "          9.4267e-02, -6.3596e-01,  1.6233e+00, -2.5041e-01, -1.5681e+00,\n",
       "          1.2616e+00,  2.6256e-01,  2.5062e-01,  1.1798e+00,  1.5179e+01,\n",
       "          1.2782e+00,  1.4662e+00,  7.8219e-01,  1.1033e+00, -3.3376e-01,\n",
       "         -2.4659e+00,  5.6272e-01,  9.8932e-01, -4.0164e-01, -1.7247e+00,\n",
       "          8.7482e-01,  2.8559e-01, -1.4991e+00,  1.8257e-01,  6.5450e-01,\n",
       "          1.4379e+00,  2.2578e+00,  1.2676e+00,  1.2480e+00, -9.0694e-01,\n",
       "         -9.7850e-01, -1.3821e-01, -4.2959e-01,  8.8256e-01, -1.8457e+00,\n",
       "          1.1700e+00,  2.8078e-01, -5.6256e-01,  1.4715e+00, -7.5645e-01,\n",
       "          2.9968e-01,  6.5087e-01,  3.2428e-01, -7.2840e-02, -1.7354e+00,\n",
       "         -7.1813e-01,  2.5838e-01,  8.1692e-01, -7.9979e-01, -1.9048e+00,\n",
       "         -4.5969e-01,  9.5149e-02,  8.2318e-01,  8.9126e-01, -3.0588e+00,\n",
       "          2.6272e+00, -8.5040e-01,  1.8972e+00, -1.9985e-02,  2.1360e-01,\n",
       "          5.0242e-01,  7.0527e-02, -7.2966e-01, -3.8574e+00,  5.6022e-01,\n",
       "         -3.6616e+00,  1.0367e-01,  5.4121e-01, -2.2387e+00, -1.6479e+00,\n",
       "         -2.5371e+00, -3.6103e-01,  1.1214e+00, -2.1517e+00, -6.5517e-01,\n",
       "         -1.3170e+00,  1.5774e+00, -5.9310e-02, -2.0554e+00,  5.9246e-01,\n",
       "          1.5215e+00, -2.3807e+00, -4.5818e-01,  5.6101e-02, -2.3689e+00,\n",
       "         -2.8592e+00,  1.1472e+00, -4.0649e-01,  1.1455e+00,  4.8433e-01,\n",
       "         -1.7302e+00, -8.5303e-01],\n",
       "        [ 1.7328e+00,  5.7675e-01, -2.4603e+00, -2.2014e+00,  8.2541e-01,\n",
       "         -1.4100e+00, -8.9367e-01,  4.2911e-01, -1.4488e+00,  1.2044e+00,\n",
       "          3.2452e-01, -8.5030e-01,  1.4306e+00,  3.4811e-01, -1.7638e+00,\n",
       "          1.4580e+00,  7.6202e-02,  1.4853e-01,  5.4905e-01,  1.4567e+01,\n",
       "          1.5846e+00,  1.5087e+00,  1.2262e+00,  8.9354e-01, -1.2033e+00,\n",
       "         -2.5406e+00,  4.9188e-01,  1.1828e+00,  6.2180e-02, -1.4690e+00,\n",
       "          7.8225e-01, -2.0441e-01, -1.7545e+00,  1.1128e-01,  7.2084e-01,\n",
       "          1.3475e+00,  2.2980e+00,  1.4380e+00,  1.1605e+00, -1.1286e+00,\n",
       "         -3.7243e-01, -1.6280e-01, -2.3222e-01,  6.3394e-01, -1.8293e+00,\n",
       "          1.0466e+00,  6.9757e-01, -3.5383e-01,  1.6474e+00, -7.4780e-01,\n",
       "          3.8942e-01,  1.0296e+00,  8.0472e-02,  2.1896e-01, -1.8383e+00,\n",
       "         -5.3796e-01, -1.9516e-01,  1.0895e+00, -1.3458e+00, -1.7897e+00,\n",
       "         -1.6350e-01,  4.9331e-01,  8.1587e-01,  1.1500e+00, -3.1207e+00,\n",
       "          2.2997e+00, -4.7442e-01,  3.0422e+00, -3.1969e-02,  9.6475e-03,\n",
       "          2.5246e-01, -1.9764e-01, -9.9523e-01, -3.4942e+00,  9.6446e-01,\n",
       "         -3.6963e+00,  6.9795e-01,  4.6191e-01, -2.4741e+00, -1.4195e+00,\n",
       "         -1.8654e+00, -3.9690e-01,  1.7965e+00, -2.7061e+00, -1.3429e+00,\n",
       "         -1.2252e+00,  1.3721e+00, -2.2891e-01, -2.1369e+00,  4.6090e-01,\n",
       "          1.5314e+00, -2.8658e+00, -1.4429e-01,  2.7626e-01, -2.1220e+00,\n",
       "         -2.9087e+00,  1.4345e+00, -6.4261e-01,  9.3853e-01,  3.7464e-01,\n",
       "         -1.0927e+00, -1.2874e-01]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 102])\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 19], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_ids = logits.argmax(dim=-1)\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Afrikaans',\n",
       " 1: 'Amharic',\n",
       " 2: 'Arabic',\n",
       " 3: 'Assamese',\n",
       " 4: 'Asturian',\n",
       " 5: 'Azerbaijani',\n",
       " 6: 'Belarusian',\n",
       " 7: 'Bulgarian',\n",
       " 8: 'Bengali',\n",
       " 9: 'Bosnian',\n",
       " 10: 'Catalan',\n",
       " 11: 'Cebuano',\n",
       " 12: 'Sorani-Kurdish',\n",
       " 13: 'Mandarin Chinese',\n",
       " 14: 'Czech',\n",
       " 15: 'Welsh',\n",
       " 16: 'Danish',\n",
       " 17: 'German',\n",
       " 18: 'Greek',\n",
       " 19: 'English',\n",
       " 20: 'Spanish',\n",
       " 21: 'Estonian',\n",
       " 22: 'Persian',\n",
       " 23: 'Fula',\n",
       " 24: 'Finnish',\n",
       " 25: 'Filipino',\n",
       " 26: 'French',\n",
       " 27: 'Irish',\n",
       " 28: 'Galician',\n",
       " 29: 'Gujarati',\n",
       " 30: 'Hausa',\n",
       " 31: 'Hebrew',\n",
       " 32: 'Hindi',\n",
       " 33: 'Croatian',\n",
       " 34: 'Hungarian',\n",
       " 35: 'Armenian',\n",
       " 36: 'Indonesian',\n",
       " 37: 'Igbo',\n",
       " 38: 'Icelandic',\n",
       " 39: 'Italian',\n",
       " 40: 'Japanese',\n",
       " 41: 'Javanese',\n",
       " 42: 'Georgian',\n",
       " 43: 'Kamba',\n",
       " 44: 'Kabuverdianu',\n",
       " 45: 'Kazakh',\n",
       " 46: 'Khmer',\n",
       " 47: 'Kannada',\n",
       " 48: 'Korean',\n",
       " 49: 'Kyrgyz',\n",
       " 50: 'Luxembourgish',\n",
       " 51: 'Ganda',\n",
       " 52: 'Lingala',\n",
       " 53: 'Lao',\n",
       " 54: 'Lithuanian',\n",
       " 55: 'Luo',\n",
       " 56: 'Latvian',\n",
       " 57: 'Maori',\n",
       " 58: 'Macedonian',\n",
       " 59: 'Malayalam',\n",
       " 60: 'Mongolian',\n",
       " 61: 'Marathi',\n",
       " 62: 'Malay',\n",
       " 63: 'Maltese',\n",
       " 64: 'Burmese',\n",
       " 65: 'Norwegian',\n",
       " 66: 'Nepali',\n",
       " 67: 'Dutch',\n",
       " 68: 'Northern-Sotho',\n",
       " 69: 'Nyanja',\n",
       " 70: 'Occitan',\n",
       " 71: 'Oromo',\n",
       " 72: 'Oriya',\n",
       " 73: 'Punjabi',\n",
       " 74: 'Polish',\n",
       " 75: 'Pashto',\n",
       " 76: 'Portuguese',\n",
       " 77: 'Romanian',\n",
       " 78: 'Russian',\n",
       " 79: 'Sindhi',\n",
       " 80: 'Slovak',\n",
       " 81: 'Slovenian',\n",
       " 82: 'Shona',\n",
       " 83: 'Somali',\n",
       " 84: 'Serbian',\n",
       " 85: 'Swedish',\n",
       " 86: 'Swahili',\n",
       " 87: 'Tamil',\n",
       " 88: 'Telugu',\n",
       " 89: 'Tajik',\n",
       " 90: 'Thai',\n",
       " 91: 'Turkish',\n",
       " 92: 'Ukrainian',\n",
       " 93: 'Umbundu',\n",
       " 94: 'Urdu',\n",
       " 95: 'Uzbek',\n",
       " 96: 'Vietnamese',\n",
       " 97: 'Wolof',\n",
       " 98: 'Xhosa',\n",
       " 99: 'Yoruba',\n",
       " 100: 'Cantonese Chinese',\n",
       " 101: 'Zulu'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "English\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label[predicted_class_ids[0].item()])\n",
    "print(model.config.id2label[predicted_class_ids[1].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
